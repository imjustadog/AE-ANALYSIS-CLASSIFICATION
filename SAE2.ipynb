{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:187: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-1-99d2f50b196f>:108: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Restoring parameters from saver/SAE2/SAE\n",
      "Step 1: Minibatch Loss: 246.459503\n",
      "Step 1000: Minibatch Loss: 259.508972\n",
      "Step 2000: Minibatch Loss: 212.244125\n",
      "Step 3000: Minibatch Loss: 238.836884\n",
      "Step 4000: Minibatch Loss: 208.406662\n",
      "Step 5000: Minibatch Loss: 199.352509\n",
      "Step 6000: Minibatch Loss: 215.802628\n",
      "Step 7000: Minibatch Loss: 194.235840\n",
      "Step 8000: Minibatch Loss: 209.054901\n",
      "Step 9000: Minibatch Loss: 187.260086\n",
      "Step 10000: Minibatch Loss: 194.118759\n",
      "Step 11000: Minibatch Loss: 176.984360\n",
      "Step 12000: Minibatch Loss: 170.137604\n",
      "Step 13000: Minibatch Loss: 162.998795\n",
      "Step 14000: Minibatch Loss: 180.561859\n",
      "Step 15000: Minibatch Loss: 168.431992\n",
      "Step 16000: Minibatch Loss: 163.401520\n",
      "Step 17000: Minibatch Loss: 177.646988\n",
      "Step 18000: Minibatch Loss: 148.986664\n",
      "Step 19000: Minibatch Loss: 154.485458\n",
      "Step 20000: Minibatch Loss: 155.823654\n",
      "Step 21000: Minibatch Loss: 155.240433\n",
      "Step 22000: Minibatch Loss: 162.367157\n",
      "Step 23000: Minibatch Loss: 147.516403\n",
      "Step 24000: Minibatch Loss: 132.922501\n",
      "Step 25000: Minibatch Loss: 132.024261\n",
      "Step 26000: Minibatch Loss: 142.034668\n",
      "Step 27000: Minibatch Loss: 134.348602\n",
      "Step 28000: Minibatch Loss: 135.419113\n",
      "Step 29000: Minibatch Loss: 130.879837\n",
      "Step 30000: Minibatch Loss: 123.410950\n",
      "Step 31000: Minibatch Loss: 131.321747\n",
      "Step 32000: Minibatch Loss: 144.214508\n",
      "Step 33000: Minibatch Loss: 118.201660\n",
      "Step 34000: Minibatch Loss: 124.571243\n",
      "Step 35000: Minibatch Loss: 123.290916\n",
      "Step 36000: Minibatch Loss: 115.285156\n",
      "Step 37000: Minibatch Loss: 127.818039\n",
      "Step 38000: Minibatch Loss: 116.830986\n",
      "Step 39000: Minibatch Loss: 128.842621\n",
      "Step 40000: Minibatch Loss: 116.177429\n",
      "Step 41000: Minibatch Loss: 119.927460\n",
      "Step 42000: Minibatch Loss: 123.680634\n",
      "Step 43000: Minibatch Loss: 105.448975\n",
      "Step 44000: Minibatch Loss: 110.373428\n",
      "Step 45000: Minibatch Loss: 106.777832\n",
      "Step 46000: Minibatch Loss: 119.572281\n",
      "Step 47000: Minibatch Loss: 101.136528\n",
      "Step 48000: Minibatch Loss: 107.445648\n",
      "Step 49000: Minibatch Loss: 97.348373\n",
      "Step 50000: Minibatch Loss: 95.093384\n",
      "Step 51000: Minibatch Loss: 122.534706\n",
      "Step 52000: Minibatch Loss: 100.503273\n",
      "Step 53000: Minibatch Loss: 105.232559\n",
      "Step 54000: Minibatch Loss: 96.609787\n",
      "Step 55000: Minibatch Loss: 95.659927\n",
      "Step 56000: Minibatch Loss: 108.503952\n",
      "Step 57000: Minibatch Loss: 96.854416\n",
      "Step 58000: Minibatch Loss: 87.590759\n",
      "Step 59000: Minibatch Loss: 92.714302\n",
      "Step 60000: Minibatch Loss: 90.625458\n",
      "Step 61000: Minibatch Loss: 92.441330\n",
      "Step 62000: Minibatch Loss: 89.872345\n",
      "Step 63000: Minibatch Loss: 99.628922\n",
      "Step 64000: Minibatch Loss: 86.185074\n",
      "Step 65000: Minibatch Loss: 91.837341\n",
      "Step 66000: Minibatch Loss: 85.694725\n",
      "Step 67000: Minibatch Loss: 80.600159\n",
      "Step 68000: Minibatch Loss: 90.373253\n",
      "Step 69000: Minibatch Loss: 79.493675\n",
      "Step 70000: Minibatch Loss: 90.003891\n",
      "Step 71000: Minibatch Loss: 82.413483\n",
      "Step 72000: Minibatch Loss: 98.750366\n",
      "Step 73000: Minibatch Loss: 86.516129\n",
      "Step 74000: Minibatch Loss: 84.365211\n",
      "Step 75000: Minibatch Loss: 80.380180\n",
      "Step 76000: Minibatch Loss: 82.950508\n",
      "Step 77000: Minibatch Loss: 80.216476\n",
      "Step 78000: Minibatch Loss: 83.832382\n",
      "Step 79000: Minibatch Loss: 83.913658\n",
      "Step 80000: Minibatch Loss: 83.480072\n",
      "Step 81000: Minibatch Loss: 86.406342\n",
      "Step 82000: Minibatch Loss: 84.808395\n",
      "Step 83000: Minibatch Loss: 77.861023\n",
      "Step 84000: Minibatch Loss: 79.049637\n",
      "Step 85000: Minibatch Loss: 78.473824\n",
      "Step 86000: Minibatch Loss: 86.098755\n",
      "Step 87000: Minibatch Loss: 74.922722\n",
      "Step 88000: Minibatch Loss: 78.446655\n",
      "Step 89000: Minibatch Loss: 74.824188\n",
      "Step 90000: Minibatch Loss: 76.580261\n",
      "Step 91000: Minibatch Loss: 91.422409\n",
      "Step 92000: Minibatch Loss: 79.793579\n",
      "Step 93000: Minibatch Loss: 75.283653\n",
      "Step 94000: Minibatch Loss: 75.104942\n",
      "Step 95000: Minibatch Loss: 71.829056\n",
      "Step 96000: Minibatch Loss: 68.543259\n",
      "Step 97000: Minibatch Loss: 67.973183\n",
      "Step 98000: Minibatch Loss: 67.627701\n",
      "Step 99000: Minibatch Loss: 79.280586\n",
      "Step 100000: Minibatch Loss: 68.180084\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "image_width=20\n",
    "\n",
    "pwd = os.getcwd()\n",
    "tfrecord_file_path = pwd + \"/AE_input_fric_ch1_20.tfrecord\"\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "                              tf.train.match_filenames_once(tfrecord_file_path),\n",
    "                              shuffle=True, num_epochs=None)\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "features = tf.parse_single_example(serialized_example,\n",
    "                                       features={'data': tf.FixedLenFeature([], tf.string)}) \n",
    "img = tf.decode_raw(features['data'], tf.uint8)\n",
    "img = tf.image.convert_image_dtype(img,tf.float32)\n",
    "img = tf.reshape(img, [image_width * image_width])\n",
    "input_batch = tf.train.shuffle_batch([img],batch_size=50,capacity=500,min_after_dequeue=250,num_threads=4)\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 100000\n",
    "display_step = 1000\n",
    "#learning_rate = tf.train.exponential_decay(0.001,num_steps,display_step,decay_rate=0.98,staircase=True)  \n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 500 # 1st layer num features\n",
    "num_input = image_width * image_width\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, num_input],name='input_x')\n",
    "\n",
    "encoder_h1=tf.Variable(tf.random_normal([num_input, num_hidden_1]))\n",
    "decoder_h1=tf.transpose(encoder_h1)\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_input])),\n",
    "}\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, encoder_h1),\n",
    "                                   biases['encoder_b1']))\n",
    "    return layer_1\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, decoder_h1),\n",
    "                                   biases['decoder_b1']))\n",
    "    return layer_1\n",
    "\n",
    "def cosine(a,b):\n",
    "    a_norm = tf.sqrt(tf.reduce_sum(tf.pow(a,2)))\n",
    "    b_norm = tf.sqrt(tf.reduce_sum(tf.pow(b,2)))\n",
    "    a_b = tf.reduce_sum(tf.multiply(a,b))\n",
    "    cosin = tf.divide(a_b,tf.multiply(a_norm,b_norm))\n",
    "    return cosin\n",
    "\n",
    "def kl_divergence(p, h): #对所有x的每个h取平均作为phat，最终所有h相加求和\n",
    "    p_hat = tf.reduce_mean(h,axis=0)\n",
    "    #p_hat = h\n",
    "    return tf.reduce_sum(p * tf.log(p)\n",
    "                            - p * tf.log(tf.clip_by_value(p_hat, 1e-8, tf.reduce_max(p_hat)))\n",
    "                            + (1 - p) * tf.log(1-p)\n",
    "                            - (1 - p) * tf.log(tf.clip_by_value(1-p_hat, 1e-8, tf.reduce_max(1-p_hat))))\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "\n",
    "#loss = tf.reduce_sum(tf.pow(y_true - y_pred, 2))\n",
    "sparsity_level = 0.01\n",
    "loss = tf.reduce_sum(tf.pow(tf.subtract(y_true, y_pred), 2.0)) + 20 * kl_divergence(sparsity_level, encoder_op)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "#loss = tf.reduce_sum(tf.subtract(tf.constant(1.0),cosine(y_true,y_pred)))\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "tf.add_to_collection('output_y', encoder_op)\n",
    "tf.add_to_collection('output_y', decoder_op)\n",
    "#tf.add_to_collection('output_y', loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tf.local_variables_initializer().run()\n",
    "    tf.global_variables_initializer().run()\n",
    "    coord = tf.train.Coordinator()\n",
    "    thread = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    \n",
    "    saver = tf.train.import_meta_graph('saver/SAE2/SAE.meta')\n",
    "    saver.restore(sess,'saver/SAE2/SAE') \n",
    "    \n",
    "    result = []\n",
    "    for i in range(1,num_steps+1):\n",
    "        _, l, h = sess.run([optimizer, loss, encoder_op],feed_dict={X: input_batch.eval()})\n",
    "        result.append(l)\n",
    "        if i % display_step == 0 or i == 1:\n",
    "            print('Step %i: Minibatch Loss: %f' % (i, l))\n",
    "            \n",
    "    saver.save(sess,'saver/SAE2/SAE')\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.rc('font',family='Times New Roman',size=10)\n",
    "plt.xlabel(\"iteration number\")\n",
    "plt.ylabel(\"loss value\")\n",
    "plt.title(\"train loss\")\n",
    "plt.plot(result)\n",
    "plt.ylim(0,50)\n",
    "plt.xlim(20000,200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train\",'w') as fr:\n",
    "    for loss in result:\n",
    "        fr.write(str(loss)+\"\\r\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
