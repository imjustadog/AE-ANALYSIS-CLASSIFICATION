{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:187: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-1-549787299b7c>:108: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Step 1: Minibatch Loss: 8991.939453\n",
      "Step 1000: Minibatch Loss: 1846.309204\n",
      "Step 2000: Minibatch Loss: 1526.106445\n",
      "Step 3000: Minibatch Loss: 1321.635498\n",
      "Step 4000: Minibatch Loss: 1222.640747\n",
      "Step 5000: Minibatch Loss: 1267.857178\n",
      "Step 6000: Minibatch Loss: 1112.568970\n",
      "Step 7000: Minibatch Loss: 979.779602\n",
      "Step 8000: Minibatch Loss: 1024.899902\n",
      "Step 9000: Minibatch Loss: 964.845337\n",
      "Step 10000: Minibatch Loss: 914.767395\n",
      "Step 11000: Minibatch Loss: 835.229004\n",
      "Step 12000: Minibatch Loss: 913.743347\n",
      "Step 13000: Minibatch Loss: 766.677185\n",
      "Step 14000: Minibatch Loss: 807.244446\n",
      "Step 15000: Minibatch Loss: 731.843384\n",
      "Step 16000: Minibatch Loss: 773.226135\n",
      "Step 17000: Minibatch Loss: 650.074219\n",
      "Step 18000: Minibatch Loss: 712.370667\n",
      "Step 19000: Minibatch Loss: 621.795715\n",
      "Step 20000: Minibatch Loss: 580.365784\n",
      "Step 21000: Minibatch Loss: 649.995850\n",
      "Step 22000: Minibatch Loss: 648.872314\n",
      "Step 23000: Minibatch Loss: 647.953247\n",
      "Step 24000: Minibatch Loss: 599.614807\n",
      "Step 25000: Minibatch Loss: 675.679321\n",
      "Step 26000: Minibatch Loss: 648.952759\n",
      "Step 27000: Minibatch Loss: 583.847046\n",
      "Step 28000: Minibatch Loss: 622.703369\n",
      "Step 29000: Minibatch Loss: 583.434875\n",
      "Step 30000: Minibatch Loss: 608.220154\n",
      "Step 31000: Minibatch Loss: 638.128784\n",
      "Step 32000: Minibatch Loss: 629.303162\n",
      "Step 33000: Minibatch Loss: 623.142151\n",
      "Step 34000: Minibatch Loss: 559.675659\n",
      "Step 35000: Minibatch Loss: 643.841919\n",
      "Step 36000: Minibatch Loss: 634.600281\n",
      "Step 37000: Minibatch Loss: 632.836426\n",
      "Step 38000: Minibatch Loss: 590.393433\n",
      "Step 39000: Minibatch Loss: 560.786560\n",
      "Step 40000: Minibatch Loss: 525.595886\n",
      "Step 41000: Minibatch Loss: 542.441467\n",
      "Step 42000: Minibatch Loss: 594.272095\n",
      "Step 43000: Minibatch Loss: 536.513184\n",
      "Step 44000: Minibatch Loss: 567.984192\n",
      "Step 45000: Minibatch Loss: 521.980347\n",
      "Step 46000: Minibatch Loss: 594.842529\n",
      "Step 47000: Minibatch Loss: 567.026611\n",
      "Step 48000: Minibatch Loss: 596.989136\n",
      "Step 49000: Minibatch Loss: 552.853088\n",
      "Step 50000: Minibatch Loss: 525.327271\n",
      "Step 51000: Minibatch Loss: 549.719360\n",
      "Step 52000: Minibatch Loss: 509.694580\n",
      "Step 53000: Minibatch Loss: 520.573853\n",
      "Step 54000: Minibatch Loss: 484.629852\n",
      "Step 55000: Minibatch Loss: 438.686127\n",
      "Step 56000: Minibatch Loss: 460.600525\n",
      "Step 57000: Minibatch Loss: 424.161621\n",
      "Step 58000: Minibatch Loss: 471.513092\n",
      "Step 59000: Minibatch Loss: 406.896240\n",
      "Step 60000: Minibatch Loss: 399.574219\n",
      "Step 61000: Minibatch Loss: 366.603882\n",
      "Step 62000: Minibatch Loss: 387.947388\n",
      "Step 63000: Minibatch Loss: 357.599121\n",
      "Step 64000: Minibatch Loss: 346.022461\n",
      "Step 65000: Minibatch Loss: 329.589905\n",
      "Step 66000: Minibatch Loss: 353.738342\n",
      "Step 67000: Minibatch Loss: 335.235016\n",
      "Step 68000: Minibatch Loss: 345.766022\n",
      "Step 69000: Minibatch Loss: 280.613831\n",
      "Step 70000: Minibatch Loss: 325.392792\n",
      "Step 71000: Minibatch Loss: 306.448883\n",
      "Step 72000: Minibatch Loss: 302.694946\n",
      "Step 73000: Minibatch Loss: 271.956726\n",
      "Step 74000: Minibatch Loss: 301.947693\n",
      "Step 75000: Minibatch Loss: 293.545227\n",
      "Step 76000: Minibatch Loss: 269.794739\n",
      "Step 77000: Minibatch Loss: 256.254333\n",
      "Step 78000: Minibatch Loss: 259.393188\n",
      "Step 79000: Minibatch Loss: 240.591003\n",
      "Step 80000: Minibatch Loss: 217.516861\n",
      "Step 81000: Minibatch Loss: 265.192505\n",
      "Step 82000: Minibatch Loss: 225.855530\n",
      "Step 83000: Minibatch Loss: 223.028290\n",
      "Step 84000: Minibatch Loss: 212.239243\n",
      "Step 85000: Minibatch Loss: 203.296875\n",
      "Step 86000: Minibatch Loss: 203.955673\n",
      "Step 87000: Minibatch Loss: 227.922363\n",
      "Step 88000: Minibatch Loss: 196.509567\n",
      "Step 89000: Minibatch Loss: 191.059570\n",
      "Step 90000: Minibatch Loss: 179.936386\n",
      "Step 91000: Minibatch Loss: 196.103119\n",
      "Step 92000: Minibatch Loss: 197.982117\n",
      "Step 93000: Minibatch Loss: 184.149307\n",
      "Step 94000: Minibatch Loss: 163.621674\n",
      "Step 95000: Minibatch Loss: 184.190842\n",
      "Step 96000: Minibatch Loss: 190.351166\n",
      "Step 97000: Minibatch Loss: 159.859497\n",
      "Step 98000: Minibatch Loss: 165.678375\n",
      "Step 99000: Minibatch Loss: 164.492279\n",
      "Step 100000: Minibatch Loss: 163.470749\n",
      "Step 101000: Minibatch Loss: 163.778809\n",
      "Step 102000: Minibatch Loss: 155.581558\n",
      "Step 103000: Minibatch Loss: 155.539185\n",
      "Step 104000: Minibatch Loss: 155.985397\n",
      "Step 105000: Minibatch Loss: 164.025681\n",
      "Step 106000: Minibatch Loss: 154.613953\n",
      "Step 107000: Minibatch Loss: 140.679382\n",
      "Step 108000: Minibatch Loss: 129.355331\n",
      "Step 109000: Minibatch Loss: 156.931427\n",
      "Step 110000: Minibatch Loss: 132.558914\n",
      "Step 111000: Minibatch Loss: 151.979218\n",
      "Step 112000: Minibatch Loss: 155.789902\n",
      "Step 113000: Minibatch Loss: 141.314178\n",
      "Step 114000: Minibatch Loss: 144.094742\n",
      "Step 115000: Minibatch Loss: 124.939438\n",
      "Step 116000: Minibatch Loss: 134.546753\n",
      "Step 117000: Minibatch Loss: 139.079422\n",
      "Step 118000: Minibatch Loss: 136.001694\n",
      "Step 119000: Minibatch Loss: 160.026154\n",
      "Step 120000: Minibatch Loss: 130.874695\n",
      "Step 121000: Minibatch Loss: 153.044968\n",
      "Step 122000: Minibatch Loss: 133.527039\n",
      "Step 123000: Minibatch Loss: 130.779816\n",
      "Step 124000: Minibatch Loss: 130.506577\n",
      "Step 125000: Minibatch Loss: 145.790756\n",
      "Step 126000: Minibatch Loss: 129.281219\n",
      "Step 127000: Minibatch Loss: 127.533218\n",
      "Step 128000: Minibatch Loss: 138.417847\n",
      "Step 129000: Minibatch Loss: 128.090836\n",
      "Step 130000: Minibatch Loss: 130.017380\n",
      "Step 131000: Minibatch Loss: 125.591537\n",
      "Step 132000: Minibatch Loss: 135.687973\n",
      "Step 133000: Minibatch Loss: 118.436394\n",
      "Step 134000: Minibatch Loss: 121.320282\n",
      "Step 135000: Minibatch Loss: 123.881638\n",
      "Step 136000: Minibatch Loss: 123.415817\n",
      "Step 137000: Minibatch Loss: 117.862663\n",
      "Step 138000: Minibatch Loss: 125.234108\n",
      "Step 139000: Minibatch Loss: 112.624069\n",
      "Step 140000: Minibatch Loss: 111.332344\n",
      "Step 141000: Minibatch Loss: 121.410347\n",
      "Step 142000: Minibatch Loss: 115.898224\n",
      "Step 143000: Minibatch Loss: 104.281227\n",
      "Step 144000: Minibatch Loss: 109.488022\n",
      "Step 145000: Minibatch Loss: 105.671837\n",
      "Step 146000: Minibatch Loss: 111.857513\n",
      "Step 147000: Minibatch Loss: 106.894936\n",
      "Step 148000: Minibatch Loss: 114.697205\n",
      "Step 149000: Minibatch Loss: 123.210983\n",
      "Step 150000: Minibatch Loss: 115.908264\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "image_width=20\n",
    "\n",
    "pwd = os.getcwd()\n",
    "tfrecord_file_path = pwd + \"/AE_input_sand_ch1.tfrecord\"\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "                              tf.train.match_filenames_once(tfrecord_file_path),\n",
    "                              shuffle=True, num_epochs=None)\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "features = tf.parse_single_example(serialized_example,\n",
    "                                       features={'data': tf.FixedLenFeature([], tf.string)}) \n",
    "img = tf.decode_raw(features['data'], tf.uint8)\n",
    "img = tf.image.convert_image_dtype(img,tf.float32)\n",
    "img = tf.reshape(img, [image_width * image_width * 2])\n",
    "input_batch = tf.train.shuffle_batch([img],batch_size=50,capacity=500,min_after_dequeue=250,num_threads=4)\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 150000\n",
    "display_step = 1000\n",
    "#learning_rate = tf.train.exponential_decay(0.001,num_steps,display_step,decay_rate=0.98,staircase=True)  \n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 256 # 1st layer num features\n",
    "num_input = image_width * image_width * 2\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, num_input],name='input_x')\n",
    "\n",
    "encoder_h1=tf.Variable(tf.random_normal([num_input, num_hidden_1]))\n",
    "decoder_h1=tf.transpose(encoder_h1)\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_input])),\n",
    "}\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, encoder_h1),\n",
    "                                   biases['encoder_b1']))\n",
    "    return layer_1\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, decoder_h1),\n",
    "                                   biases['decoder_b1']))\n",
    "    return layer_1\n",
    "\n",
    "def cosine(a,b):\n",
    "    a_norm = tf.sqrt(tf.reduce_sum(tf.pow(a,2)))\n",
    "    b_norm = tf.sqrt(tf.reduce_sum(tf.pow(b,2)))\n",
    "    a_b = tf.reduce_sum(tf.multiply(a,b))\n",
    "    cosin = tf.divide(a_b,tf.multiply(a_norm,b_norm))\n",
    "    return cosin\n",
    "\n",
    "def kl_divergence(p, h): #对所有x的每个h取平均作为phat，最终所有h相加求和\n",
    "    p_hat = tf.reduce_mean(h,axis=0)\n",
    "    #p_hat = h\n",
    "    return tf.reduce_sum(p * tf.log(p)\n",
    "                            - p * tf.log(tf.clip_by_value(p_hat, 1e-8, tf.reduce_max(p_hat)))\n",
    "                            + (1 - p) * tf.log(1-p)\n",
    "                            - (1 - p) * tf.log(tf.clip_by_value(1-p_hat, 1e-8, tf.reduce_max(1-p_hat))))\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "\n",
    "#loss = tf.reduce_sum(tf.pow(y_true - y_pred, 2))\n",
    "sparsity_level = 0.05\n",
    "loss = tf.reduce_sum(tf.pow(tf.subtract(y_true, y_pred), 2.0))+kl_divergence(sparsity_level, encoder_op)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "#loss = tf.reduce_sum(tf.subtract(tf.constant(1.0),cosine(y_true,y_pred)))\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "tf.add_to_collection('output_y', encoder_op)\n",
    "tf.add_to_collection('output_y', decoder_op)\n",
    "#tf.add_to_collection('output_y', loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tf.local_variables_initializer().run()\n",
    "    tf.global_variables_initializer().run()\n",
    "    coord = tf.train.Coordinator()\n",
    "    thread = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    \n",
    "    #saver = tf.train.import_meta_graph('saver/SAE2/SAE.meta')\n",
    "    #saver.restore(sess,'saver/SAE2/SAE') \n",
    "    \n",
    "    result = []\n",
    "    for i in range(1,num_steps+1):\n",
    "        _, l, h = sess.run([optimizer, loss, encoder_op],feed_dict={X: input_batch.eval()})\n",
    "        result.append(l)\n",
    "        if i % display_step == 0 or i == 1:\n",
    "            print('Step %i: Minibatch Loss: %f' % (i, l))\n",
    "            \n",
    "    saver.save(sess,'saver/SAE2/SAE')\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 200000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/matplotlib/font_manager.py:1328: UserWarning: findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rc('font',family='Times New Roman',size=10)\n",
    "plt.xlabel(\"iteration number\")\n",
    "plt.ylabel(\"loss value\")\n",
    "plt.title(\"train loss\")\n",
    "plt.plot(result)\n",
    "plt.ylim(0,50)\n",
    "plt.xlim(20000,200000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
