{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:187: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-1-3c609b622f6a>:79: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Step 1: Minibatch Loss: 1943.784546\n",
      "Step 1000: Minibatch Loss: 8.818274\n",
      "Step 2000: Minibatch Loss: 7.720947\n",
      "Step 3000: Minibatch Loss: 8.222894\n",
      "Step 4000: Minibatch Loss: 4.846938\n",
      "Step 5000: Minibatch Loss: 7.609851\n",
      "Step 6000: Minibatch Loss: 4.137058\n",
      "Step 7000: Minibatch Loss: 5.596507\n",
      "Step 8000: Minibatch Loss: 5.102387\n",
      "Step 9000: Minibatch Loss: 5.566352\n",
      "Step 10000: Minibatch Loss: 5.422500\n",
      "Step 11000: Minibatch Loss: 6.823655\n",
      "Step 12000: Minibatch Loss: 4.370588\n",
      "Step 13000: Minibatch Loss: 4.783087\n",
      "Step 14000: Minibatch Loss: 8.497281\n",
      "Step 15000: Minibatch Loss: 3.635842\n",
      "Step 16000: Minibatch Loss: 4.040569\n",
      "Step 17000: Minibatch Loss: 4.462248\n",
      "Step 18000: Minibatch Loss: 4.828103\n",
      "Step 19000: Minibatch Loss: 5.625958\n",
      "Step 20000: Minibatch Loss: 5.294112\n",
      "Step 21000: Minibatch Loss: 5.914968\n",
      "Step 22000: Minibatch Loss: 5.349417\n",
      "Step 23000: Minibatch Loss: 4.809916\n",
      "Step 24000: Minibatch Loss: 4.902045\n",
      "Step 25000: Minibatch Loss: 5.122607\n",
      "Step 26000: Minibatch Loss: 4.396573\n",
      "Step 27000: Minibatch Loss: 6.815380\n",
      "Step 28000: Minibatch Loss: 6.468060\n",
      "Step 29000: Minibatch Loss: 4.376647\n",
      "Step 30000: Minibatch Loss: 5.785978\n",
      "Step 31000: Minibatch Loss: 4.224767\n",
      "Step 32000: Minibatch Loss: 5.147758\n",
      "Step 33000: Minibatch Loss: 4.381276\n",
      "Step 34000: Minibatch Loss: 4.072980\n",
      "Step 35000: Minibatch Loss: 4.732500\n",
      "Step 36000: Minibatch Loss: 6.921572\n",
      "Step 37000: Minibatch Loss: 5.673223\n",
      "Step 38000: Minibatch Loss: 3.634006\n",
      "Step 39000: Minibatch Loss: 7.116294\n",
      "Step 40000: Minibatch Loss: 6.622152\n",
      "Step 41000: Minibatch Loss: 5.612404\n",
      "Step 42000: Minibatch Loss: 5.478328\n",
      "Step 43000: Minibatch Loss: 5.428678\n",
      "Step 44000: Minibatch Loss: 4.351230\n",
      "Step 45000: Minibatch Loss: 5.342588\n",
      "Step 46000: Minibatch Loss: 4.731025\n",
      "Step 47000: Minibatch Loss: 3.620153\n",
      "Step 48000: Minibatch Loss: 3.711482\n",
      "Step 49000: Minibatch Loss: 3.815350\n",
      "Step 50000: Minibatch Loss: 3.990677\n",
      "Step 51000: Minibatch Loss: 4.231898\n",
      "Step 52000: Minibatch Loss: 6.636438\n",
      "Step 53000: Minibatch Loss: 4.812098\n",
      "Step 54000: Minibatch Loss: 4.595336\n",
      "Step 55000: Minibatch Loss: 3.752224\n",
      "Step 56000: Minibatch Loss: 4.878511\n",
      "Step 57000: Minibatch Loss: 3.417820\n",
      "Step 58000: Minibatch Loss: 5.980597\n",
      "Step 59000: Minibatch Loss: 3.785655\n",
      "Step 60000: Minibatch Loss: 5.509914\n",
      "Step 61000: Minibatch Loss: 2.833176\n",
      "Step 62000: Minibatch Loss: 4.195546\n",
      "Step 63000: Minibatch Loss: 4.688875\n",
      "Step 64000: Minibatch Loss: 4.497052\n",
      "Step 65000: Minibatch Loss: 4.329219\n",
      "Step 66000: Minibatch Loss: 5.369900\n",
      "Step 67000: Minibatch Loss: 3.658956\n",
      "Step 68000: Minibatch Loss: 3.677398\n",
      "Step 69000: Minibatch Loss: 4.288857\n",
      "Step 70000: Minibatch Loss: 4.548876\n",
      "Step 71000: Minibatch Loss: 4.835986\n",
      "Step 72000: Minibatch Loss: 3.167188\n",
      "Step 73000: Minibatch Loss: 3.877728\n",
      "Step 74000: Minibatch Loss: 4.498475\n",
      "Step 75000: Minibatch Loss: 2.743771\n",
      "Step 76000: Minibatch Loss: 4.551070\n",
      "Step 77000: Minibatch Loss: 3.724259\n",
      "Step 78000: Minibatch Loss: 5.031612\n",
      "Step 79000: Minibatch Loss: 4.165998\n",
      "Step 80000: Minibatch Loss: 4.406563\n",
      "Step 81000: Minibatch Loss: 4.567943\n",
      "Step 82000: Minibatch Loss: 3.283019\n",
      "Step 83000: Minibatch Loss: 5.561252\n",
      "Step 84000: Minibatch Loss: 3.142839\n",
      "Step 85000: Minibatch Loss: 6.864048\n",
      "Step 86000: Minibatch Loss: 2.940748\n",
      "Step 87000: Minibatch Loss: 3.639652\n",
      "Step 88000: Minibatch Loss: 3.838056\n",
      "Step 89000: Minibatch Loss: 4.048704\n",
      "Step 90000: Minibatch Loss: 2.647737\n",
      "Step 91000: Minibatch Loss: 4.605575\n",
      "Step 92000: Minibatch Loss: 3.070955\n",
      "Step 93000: Minibatch Loss: 4.924850\n",
      "Step 94000: Minibatch Loss: 3.628151\n",
      "Step 95000: Minibatch Loss: 3.935588\n",
      "Step 96000: Minibatch Loss: 3.894500\n",
      "Step 97000: Minibatch Loss: 4.801330\n",
      "Step 98000: Minibatch Loss: 4.102445\n",
      "Step 99000: Minibatch Loss: 4.158515\n",
      "Step 100000: Minibatch Loss: 4.465008\n",
      "Step 101000: Minibatch Loss: 3.560078\n",
      "Step 102000: Minibatch Loss: 3.106053\n",
      "Step 103000: Minibatch Loss: 4.859813\n",
      "Step 104000: Minibatch Loss: 4.062273\n",
      "Step 105000: Minibatch Loss: 2.917414\n",
      "Step 106000: Minibatch Loss: 4.032545\n",
      "Step 107000: Minibatch Loss: 4.031393\n",
      "Step 108000: Minibatch Loss: 2.820126\n",
      "Step 109000: Minibatch Loss: 4.162373\n",
      "Step 110000: Minibatch Loss: 3.459347\n",
      "Step 111000: Minibatch Loss: 5.569068\n",
      "Step 112000: Minibatch Loss: 4.056990\n",
      "Step 113000: Minibatch Loss: 3.498538\n",
      "Step 114000: Minibatch Loss: 3.718387\n",
      "Step 115000: Minibatch Loss: 4.031113\n",
      "Step 116000: Minibatch Loss: 3.604609\n",
      "Step 117000: Minibatch Loss: 6.410320\n",
      "Step 118000: Minibatch Loss: 3.383124\n",
      "Step 119000: Minibatch Loss: 2.573859\n",
      "Step 120000: Minibatch Loss: 4.258883\n",
      "Step 121000: Minibatch Loss: 3.781701\n",
      "Step 122000: Minibatch Loss: 4.849043\n",
      "Step 123000: Minibatch Loss: 5.710400\n",
      "Step 124000: Minibatch Loss: 3.224740\n",
      "Step 125000: Minibatch Loss: 2.887228\n",
      "Step 126000: Minibatch Loss: 4.085944\n",
      "Step 127000: Minibatch Loss: 2.846149\n",
      "Step 128000: Minibatch Loss: 5.590369\n",
      "Step 129000: Minibatch Loss: 3.037422\n",
      "Step 130000: Minibatch Loss: 3.801525\n",
      "Step 131000: Minibatch Loss: 3.025139\n",
      "Step 132000: Minibatch Loss: 3.295313\n",
      "Step 133000: Minibatch Loss: 3.493221\n",
      "Step 134000: Minibatch Loss: 4.405967\n",
      "Step 135000: Minibatch Loss: 3.920027\n",
      "Step 136000: Minibatch Loss: 3.033475\n",
      "Step 137000: Minibatch Loss: 4.173151\n",
      "Step 138000: Minibatch Loss: 6.541656\n",
      "Step 139000: Minibatch Loss: 3.678552\n",
      "Step 140000: Minibatch Loss: 3.116240\n",
      "Step 141000: Minibatch Loss: 5.041012\n",
      "Step 142000: Minibatch Loss: 2.563568\n",
      "Step 143000: Minibatch Loss: 2.186495\n",
      "Step 144000: Minibatch Loss: 3.893713\n",
      "Step 145000: Minibatch Loss: 2.746757\n",
      "Step 146000: Minibatch Loss: 2.413344\n",
      "Step 147000: Minibatch Loss: 3.772972\n",
      "Step 148000: Minibatch Loss: 2.635049\n",
      "Step 149000: Minibatch Loss: 3.742919\n",
      "Step 150000: Minibatch Loss: 3.294066\n",
      "Step 151000: Minibatch Loss: 2.604360\n",
      "Step 152000: Minibatch Loss: 2.512427\n",
      "Step 153000: Minibatch Loss: 3.345235\n",
      "Step 154000: Minibatch Loss: 3.236580\n",
      "Step 155000: Minibatch Loss: 2.508784\n",
      "Step 156000: Minibatch Loss: 3.450776\n",
      "Step 157000: Minibatch Loss: 2.709561\n",
      "Step 158000: Minibatch Loss: 2.845783\n",
      "Step 159000: Minibatch Loss: 3.098879\n",
      "Step 160000: Minibatch Loss: 2.290292\n",
      "Step 161000: Minibatch Loss: 3.347168\n",
      "Step 162000: Minibatch Loss: 2.945904\n",
      "Step 163000: Minibatch Loss: 3.774302\n",
      "Step 164000: Minibatch Loss: 2.453382\n",
      "Step 165000: Minibatch Loss: 2.799269\n",
      "Step 166000: Minibatch Loss: 2.195719\n",
      "Step 167000: Minibatch Loss: 3.103857\n",
      "Step 168000: Minibatch Loss: 3.499007\n",
      "Step 169000: Minibatch Loss: 2.729408\n",
      "Step 170000: Minibatch Loss: 3.153114\n",
      "Step 171000: Minibatch Loss: 2.157577\n",
      "Step 172000: Minibatch Loss: 2.725413\n",
      "Step 173000: Minibatch Loss: 2.696836\n",
      "Step 174000: Minibatch Loss: 3.071327\n",
      "Step 175000: Minibatch Loss: 2.301407\n",
      "Step 176000: Minibatch Loss: 2.798838\n",
      "Step 177000: Minibatch Loss: 1.570133\n",
      "Step 178000: Minibatch Loss: 2.536876\n",
      "Step 179000: Minibatch Loss: 2.109214\n",
      "Step 180000: Minibatch Loss: 2.209332\n",
      "Step 181000: Minibatch Loss: 1.495292\n",
      "Step 182000: Minibatch Loss: 2.800116\n",
      "Step 183000: Minibatch Loss: 1.986966\n",
      "Step 184000: Minibatch Loss: 2.090813\n",
      "Step 185000: Minibatch Loss: 2.180450\n",
      "Step 186000: Minibatch Loss: 2.621925\n",
      "Step 187000: Minibatch Loss: 2.239244\n",
      "Step 188000: Minibatch Loss: 2.318673\n",
      "Step 189000: Minibatch Loss: 1.750546\n",
      "Step 190000: Minibatch Loss: 2.685735\n",
      "Step 191000: Minibatch Loss: 2.905320\n",
      "Step 192000: Minibatch Loss: 1.957876\n",
      "Step 193000: Minibatch Loss: 1.622424\n",
      "Step 194000: Minibatch Loss: 1.816539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 195000: Minibatch Loss: 1.740064\n",
      "Step 196000: Minibatch Loss: 1.766695\n",
      "Step 197000: Minibatch Loss: 2.170371\n",
      "Step 198000: Minibatch Loss: 2.648360\n",
      "Step 199000: Minibatch Loss: 1.978341\n",
      "Step 200000: Minibatch Loss: 1.927590\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "pwd = os.getcwd()\n",
    "tfrecord_file_path = pwd + \"/AE_input.tfrecord\"\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "                              tf.train.match_filenames_once(tfrecord_file_path),\n",
    "                              shuffle=True, num_epochs=None)\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "features = tf.parse_single_example(serialized_example,\n",
    "                                       features={'data': tf.FixedLenFeature([311], tf.float32)})  \n",
    "fft_batch = tf.train.shuffle_batch([features['data']],batch_size=20,capacity=500,min_after_dequeue=250,num_threads=1)\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 200000\n",
    "display_step = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 64 # 1st layer num features\n",
    "num_input = 311\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, num_input],name='input_x')\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_input])),\n",
    "}\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    return layer_1\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    return layer_1\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "loss = tf.reduce_sum(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "tf.add_to_collection('output_y', decoder_op)\n",
    "tf.add_to_collection('output_y', loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tf.local_variables_initializer().run()\n",
    "    tf.global_variables_initializer().run()\n",
    "    coord = tf.train.Coordinator()\n",
    "    thread = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    \n",
    "    for i in range(1,num_steps+1):\n",
    "        _, l = sess.run([optimizer, loss],feed_dict={X: fft_batch.eval()})\n",
    "        if i % display_step == 0 or i == 1:\n",
    "            print('Step %i: Minibatch Loss: %f' % (i, l))\n",
    "    \n",
    "    saver.save(sess,'saver/AE/AE')\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(thread)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
